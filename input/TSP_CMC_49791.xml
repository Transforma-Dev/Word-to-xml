<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="style/jpub3-html-trans.xsl"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN" "D:/Localserver/Apache/htdocs/Transforma/ce_editor/Conversion/Word2HTML/DTD/jats-publishing-dtd-1.0/JATS-journalpublishing1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="1.0">
<front>
<journal-meta>
<journal-id journal-id-type="pmc">CMC</journal-id>
<journal-id journal-id-type="nlm-ta">CMC</journal-id>
<journal-id journal-id-type="publisher-id">CMC</journal-id>
<journal-title-group>
<journal-title>Computers, Materials &#x0026; Continua</journal-title>
</journal-title-group>
<issn pub-type="epub">1546-2226</issn>
<issn pub-type="ppub">1546-2218</issn>
<publisher>
<publisher-name>Tech Science Press</publisher-name>
<publisher-loc>USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">49791</article-id>
<article-id pub-id-type="doi">10.32604/cmc.2024.049791</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>ARTICLE</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>CrossLinkNet: An Explainable and Trustworthy AI Framework for Whole-Slide Images Segmentation</article-title>
<alt-title alt-title-type="left-running-head">CrossLinkNet: An Explainable and Trustworthy AI Framework for Whole-Slide Images Segmentation</alt-title>
<alt-title alt-title-type="right-running-head">CrossLinkNet: An Explainable and Trustworthy AI Framework for Whole-Slide Images Segmentation</alt-title>
</title-group>
<contrib-group content-type="authors">
<contrib id="author-1" contrib-type="author">
<name name-style="western"><surname>Xiao</surname><given-names>Peng</given-names></name><xref ref-type="aff" rid="aff-1">1</xref></contrib>
<contrib id="author-2" contrib-type="author">
<name name-style="western"><surname>Zhong</surname><given-names>Qi</given-names></name><xref ref-type="aff" rid="aff-2">2</xref></contrib>
<contrib id="author-3" contrib-type="author">
<name name-style="western"><surname>Chen</surname><given-names>Jingxue</given-names></name><xref ref-type="aff" rid="aff-1">1</xref></contrib>
<contrib id="author-4" contrib-type="author">
<name name-style="western"><surname>Wu</surname><given-names>Dongyuan</given-names></name><xref ref-type="aff" rid="aff-1">1</xref></contrib>
<contrib id="author-5" contrib-type="author">
<name name-style="western"><surname>Qin</surname><given-names>Zhen</given-names></name><xref ref-type="aff" rid="aff-1">1</xref></contrib>
<contrib id="author-6" contrib-type="author" corresp="yes">
<name name-style="western"><surname>Zhou</surname><given-names>Erqiang</given-names></name><xref ref-type="aff" rid="aff-1">1</xref><email>zhoueq@uestc.edu.cn</email></contrib>
<aff id="aff-1"><label>1</label><institution>Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China</institution>, <addr-line>Chengdu, 610054</addr-line>, <country>China</country></aff>
<aff id="aff-2"><label>2</label><institution>Faculty of Data Science, City University of Macau</institution>, <addr-line>Macau, 999078</addr-line>, <country>China</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1">&#x002A;Corresponding Author: Erqiang Zhou. Email: <email>zhoueq@uestc.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub" date-type="pub" iso-8601-date="2024-04-30">
<day>30</day>
<month>xxx</month>
<year>2024</year>
</pub-date>
<volume>XX</volume>
<issue>XX</issue>
<fpage>XX</fpage>
<lpage>XX</lpage>
<history>
<date date-type="received">
<day>17</day>
<month>1</month>
<year>2024</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>4</month>
<year>2024</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2024 Xiao et al.</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>Xiao et al.</copyright-holder>
<license xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="TSP_CMC_49791.pdf"></self-uri>
<abstract>
<p>In the intelligent medical diagnosis area, Artificial Intelligence (AI)&#x2019;s trustworthiness, reliability, and interpretability are critical, especially in cancer diagnosis. Traditional neural networks, while excellent at processing natural images, often lack interpretability and adaptability when processing high-resolution digital pathological images. This limitation is particularly evident in pathological diagnosis, which is the gold standard of cancer diagnosis and relies on a pathologist&#x2019;s careful examination and analysis of digital pathological slides to identify the features and progression of the disease. Therefore, the integration of interpretable AI into smart medical diagnosis is not only an inevitable technological trend but also a key to improving diagnostic accuracy and reliability. In this paper, we introduce an innovative Multi-Scale Multi-Branch Feature Encoder (MSBE) and present the design of the CrossLinkNet Framework. The MSBE enhances the network&#x2019;s capability for feature extraction by allowing the adjustment of hyperparameters to configure the number of branches and modules. The CrossLinkNet Framework, serving as a versatile image segmentation network architecture, employs cross-layer encoder-decoder connections for multi-level feature fusion, thereby enhancing feature integration and segmentation accuracy. Comprehensive quantitative and qualitative experiments on two datasets demonstrate that CrossLinkNet, equipped with the MSBE encoder, not only achieves accurate segmentation results but is also adaptable to various tumor segmentation tasks and scenarios by replacing different feature encoders. Crucially, CrossLinkNet emphasizes the interpretability of the AI model, a crucial aspect for medical professionals, providing an in-depth understanding of the model&#x2019;s decisions and thereby enhancing trust and reliability in AI-assisted diagnostics.</p>
</abstract>
<kwd-group kwd-group-type="author">
<kwd>Explainable AI</kwd>
<kwd>security</kwd>
<kwd>trustworthy</kwd>
<kwd>CrossLinkNet</kwd>
<kwd>whole slide images</kwd>
</kwd-group>
<counts>
<page-count count="0"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>The integration of AI into clinical practice is transforming the healthcare landscape, particularly in diagnostic pathology [<xref ref-type="bibr" rid="ref-1">1</xref>]. Accurate diagnosis is the cornerstone of effective cancer treatment, where pathologists often face the daunting task of deciphering complex patterns in whole-slide images (WSIs) to identify malignancies [<xref ref-type="bibr" rid="ref-2">2</xref>]. However, this traditional process is not without its challenges. It is labor-intensive, subject to inter-observer variability, and requires significant expertise, which may be limited in resource-constrained environments [<xref ref-type="bibr" rid="ref-3">3</xref>]. As AI technology, particularly deep learning, becomes increasingly capable of augmenting or even surpassing human performance in image recognition tasks, its potential in enhancing the speed, accuracy, and efficiency of cancer diagnosis is becoming more apparent. However, alongside these advancements, concerns have emerged about the opaque nature of AI decisions and their implications for patient safety and treatment outcomes [<xref ref-type="bibr" rid="ref-4">4</xref>].</p>
<p>In the sensitive realm of oncology, the trust in AI systems hinges on their ability to not only perform with high accuracy but also provide clarity on how conclusions are drawn [<xref ref-type="bibr" rid="ref-5">5</xref>]. Traditional neural networks, while proficient in various applications, often operate as &#x201C;black boxes,&#x201D; offering little insight into their internal decision-making processes [<xref ref-type="bibr" rid="ref-6">6</xref>]. This lack of transparency is particularly problematic in the clinical setting, where understanding the rationale behind diagnostic decisions is imperative for clinician acceptance, quality control, and ethical considerations [<xref ref-type="bibr" rid="ref-7">7</xref>]. In this context, the role of explainable AI (XAI) technologies becomes pivotal, as they bridge the gap between AI performance and human interpretability. XAI aims to make the workings of complex neural networks accessible and comprehensible, enabling clinicians to follow the AI&#x2019;s logic, verify its reasoning, and, crucially, trust its outputs.</p>
<p>With the proliferation of digital pathology, WSIs have emerged as a standard in modern cancer diagnostics, providing an exhaustive view of tissue samples [<xref ref-type="bibr" rid="ref-8">8</xref>]. However, manual analysis of WSIs is notably laborious, time-consuming, and subject to variability due to human interpretation [<xref ref-type="bibr" rid="ref-9">9</xref>]. To mitigate these challenges, there has been a growing impetus towards incorporating explainable AI models. These models not only enhance the efficiency and accuracy of pathological analyses but also contribute to a deeper understanding of disease characteristics and progression [<xref ref-type="bibr" rid="ref-10">10</xref>]. AI&#x2019;s proficiency in rapidly processing and analyzing large volumes of data has been transformative across various sectors, particularly in medical imaging [<xref ref-type="bibr" rid="ref-11">11</xref>]. In the field of pathology, AI algorithms are increasingly recognized for their potential to improve WSI interpretation, providing results that are more consistent and reproducible [<xref ref-type="bibr" rid="ref-12">12</xref>,<xref ref-type="bibr" rid="ref-13">13</xref>].</p>
<p>Traditional machine learning approaches for image analysis predominantly depend on manually extracted and defined features [<xref ref-type="bibr" rid="ref-14">14</xref>]. Traditional neural network approaches encounter significant challenges in medical image segmentation, particularly with whole-slide images (WSIs). Firstly, Whole Slide Images (WSIs) typically contain gigabytes of pixels, making it impossible for traditional deep neural networks to process them directly due to computational and memory constraints. This severely limits the ability to analyze WSIs comprehensively, affecting the extraction of critical diagnostic information. Furthermore, WSIs often contain tumor regions that are very small relative to the overall image size, presenting significant challenges to conventional approaches in identifying tumors and segmenting crucial minor targets. Additionally, the rigid architecture of numerous traditional neural networks, due to the lack of a modular design, restricts their adaptability. This limitation obstructs the integration of alternative feature encoders that could potentially enhance feature extraction for specific types of medical images. Lastly, the complexity of these traditional network architectures often lacks interpretability. Within clinical contexts, grasping the fundamental rationales behind diagnostic forecasts is essential for fostering trust and guiding clinical decisions.</p>
<p>Addressing the limitations of traditional neural network approaches, this paper introduces CrossLinkNet, which is specifically designed to address these challenges through its scalable processing of high-resolution images, enhanced sensitivity to small target regions, modular architecture for flexible feature encoding, and improved interpretability, thereby offering a significant advancement in the field of medical image segmentation.</p>
<p>The primary contributions of this paper are summarized as follows:
<list list-type="bullet">
<list-item>
<p>An explainable Multi-Scale Multi-Branch Feature Encoder is proposed to enhance the network&#x2019;s capability for feature extraction. The MSBE Encoder facilitates the fine-tuning of hyperparameters to customize the quantity of branches and modules. This provides flexibility in adjusting the width of the network, thereby enhancing its capability to extract features.</p></list-item>
<list-item>
<p>A versatile medical image segmentation network architecture, CrossLinkNet, is designed to accurately identify tumor regions by leveraging cross-layer encoder-decoder connections for multi-level feature fusion, thereby enhancing feature integration and improving segmentation accuracy. Moreover, this architecture can be flexibly adapted to various tumor segmentation tasks and scenarios through the replacement of different feature encoders.</p></list-item>
<list-item>
<p>The CrossLinkNet with an MSBE encoder demonstrates accurate segmentation results through comprehensive quantitative and qualitative experiments on two datasets, namely BOT and Kvasir. Moreover, it exhibits adaptability to various tumor segmentation tasks and scenarios by seamlessly replacing different feature encoders.</p></list-item>
</list></p>
<p>The structure of this paper is organized as follows: <xref ref-type="sec" rid="s2">Section 2</xref> provides a concise review of existing literature pertinent to pathological image analysis. <xref ref-type="sec" rid="s3">Section 3</xref> elaborates on the deep neural network proposed for pathological image segmentation. <xref ref-type="sec" rid="s4">Section 4</xref> outlines the experimental setup. <xref ref-type="sec" rid="s5">Section 5</xref> presents and analyses the experimental results obtained. <xref ref-type="sec" rid="s6">Section 6</xref> discusses the potential challenges of the proposed approach. Finnally, <xref ref-type="sec" rid="s7">Section 7</xref> offers the conclusion.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Related Works</title>
<p>In the field of image segmentation, numerous methods have been developed over the years. Traditional segmentation techniques, such as threshold-based methods and edge detection algorithms, have laid the foundation in this domain. This section explores the evolution and intricacies of these traditional segmentation approaches and deep learning approaches.</p>
<sec id="s2_1">
<label>2.1</label>
<title>Threshold Segmentation Method</title>
<p>Threshold-based image segmentation is a common technique in digital image processing and an important method for achieving image segmentation [<xref ref-type="bibr" rid="ref-15">15</xref>]. The Otsu algorithm, also known as Otsu&#x2019;s method, represents a classical approach to image thresholding, initially introduced by Nobuyuki Otsu in 1979 [<xref ref-type="bibr" rid="ref-16">16</xref>]. Zhu et al. [<xref ref-type="bibr" rid="ref-17">17</xref>] developed a novel segmentation algorithm to overcome the limitations of traditional threshold-based techniques in extracting complex information. This method employs adaptive thresholds for each pixel, determined by the mean and variance of adjacent pixel values, thereby enhancing edge detection. Yang et al. [<xref ref-type="bibr" rid="ref-18">18</xref>] focused on enhancing the Otsu algorithm by examining the relationship between pixel grayscale values and cumulative pixel counts. The use of threshold-based image segmentation methods is common in digital image processing; however, they have notable limitations. One major drawback is the criticality of selecting an appropriate threshold, as different thresholds can lead to divergent segmentation outcomes.</p>
</sec>
<sec id="s2_2">
<label>2.2</label>
<title>Segmentation Method based on an Interpretable Convolutional Neural Network</title>
<p>With the rapid advancement in the fields of computer vision and deep learning [<xref ref-type="bibr" rid="ref-19">19</xref>], Convolutional Neural Networks (CNNs) have emerged as a pivotal component for various visual tasks, particularly excelling in image segmentation. Kiran et al. [<xref ref-type="bibr" rid="ref-20">20</xref>] utilized ResNet for segmenting cell clusters in whole-slide cervical pathology images. Their model achieved an impressive accuracy of 99.63% on single-cell images and 96.37% on cell clusters within entire images. Lin et al. [<xref ref-type="bibr" rid="ref-21">21</xref>] introduced a deep convolutional neural network-based lesion detection system, optimized for high efficiency and sensitivity across various lesion sizes. CNN-based image segmentation algorithms show high efficiency in detecting tumors in pathological images. However, they struggle to interpret the contextual relationships among image segments, and there&#x2019;s a considerable gap in AI interpretability.</p>
</sec>
<sec id="s2_3">
<label>2.3</label>
<title>Segmentation Method based on Transformer</title>
<p>The Vision Transformer (ViT) [<xref ref-type="bibr" rid="ref-22">22</xref>] represents a technological turning point in the field of computer vision, as it applies Transformer technology to this domain. Chen et al. [<xref ref-type="bibr" rid="ref-23">23</xref>] developed a Multimodal Co-Attention Transformer (MCAT) framework, leveraging Transformer layers for multi-instance learning to map relationships between pathological image features and genomic data. Yin et al. [<xref ref-type="bibr" rid="ref-24">24</xref>] developed the PyT2T-ViT, a streamlined Vision Transformer architecture for multi-instance learning. Transformer-based approaches demonstrate significant advantages in capturing long-range dependencies and integrating global and local information. However, their extensive model parameters require substantial hardware resources, which can result in overfitting when dealing with limited data.</p>
<p>The exploration of various segmentation methods in pathology imaging reveals a dynamic progression from traditional to advanced deep learning techniques. All the methods previously discussed have their limitations. With the advancement of this field, overcoming these weaknesses is essential for creating more efficient, accurate, and interpretable segmentation methods for pathological analysis.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Method</title>
<p>In this paper, a universal and explainable segmentation network named CrossLinkNet is proposed, with the specific details of the network shown in <?A3B2 "fig1",5,"anchor"?><xref ref-type="fig" rid="fig-1">Fig. 1</xref>. CrossLinkNet consists of the MSB Encoder and the CrossLinkNet Framework. The MSB Encoder is a multi-scale, multi-branch light-weight feature encoder designed for extracting multi-scale features, with its architecture illustrated in <?A3B2 "fig2",5,"anchor"?><xref ref-type="fig" rid="fig-2">Fig. 2</xref>. <?A3B2 "fig3",5,"anchor"?><xref ref-type="fig" rid="fig-3">Figure 3</xref> demonstrates the structure of the MSB Encoder in various modes. The CrossLinkNet Framework is a generic segmentation framework based on an encoder-decoder architecture with cross-layer connections, as shown in <?A3B2 "fig4",5,"anchor"?><xref ref-type="fig" rid="fig-4">Fig. 4</xref>. By combining the CrossLinkNet Framework with the MSB Encoder, it is possible to identify more features of tumors and perform pixel-level segmentation. By substituting different feature encoders, the network can also be adapted to various medical imaging analysis tasks, ultimately achieving commendable results in segmentation accuracy. The following sections will provide detailed introductions to the Multi-Scale Multi-Branch Feature Encoder (MSBE) and the generic segmentation network architecture, CrossLinkNet.</p>
<fig id="fig-1">
<label>Figure 1</label>
<caption>
<title>CrossLinkNet architecture</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-1.tif"/>
</fig>
<fig id="fig-2">
<label>Figure 2</label>
<caption>
<title>Basic structure of MSB encoder</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-2.tif"/>
</fig>
<fig id="fig-3">
<label>Figure 3</label>
<caption>
<title>Different structure of MSB encoder</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-3.tif"/>
</fig>
<fig id="fig-4">
<label>Figure 4</label>
<caption>
<title>CrossLinkNet segmentation network framework</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-4.tif"/>
</fig>
<sec id="s3_1">
<label>3.1</label>
<title>Multi-Scale Multi-Branch Feature Encoder</title>
<p>The extraction of multi-scale features plays a pivotal role in neural networks, particularly in tasks such as object detection and image segmentation, where it can significantly enhance the performance of the model. Traditional convolutional neural networks employ standard convolutional kernels and pooling operations of fixed sizes to extract features from images. However, this approach predominantly captures local features and is limited in its capacity to handle objects of varying sizes. To address this limitation, this section introduces a MSB encoder for feature extraction within the proposed versatile segmentation network, CrossLinkNet. The fundamental architecture of this feature encoder is depicted in <xref ref-type="fig" rid="fig-2">Fig. 2</xref>.</p>
<p>The architecture of the MSB Encoder primarily encompasses dimensionality expansion, depthwise separable convolutions, multi-scale feature extraction, dimensionality reduction and skip connections. The MSB encoder initially employs a strategy of increasing dimensionality (using 1 &#x00D7; 1 convolutions), followed by feature extraction, and culminating in a reduction of dimensions. This approach primarily aims to bolster the network&#x2019;s ability to represent complex features. Elevating the dimensionality allows the network to learn features in a higher-dimensional space, which facilitates capturing more complex feature relationships. The dimension reduction phase that follows is designed to curtail the parameter count, alleviate overfitting, and ensure the network remains efficient. The adoption of depthwise separable convolutions is based on their advantage of reducing computations and model size while still maintaining good feature extraction performance. By decomposing traditional convolutions into depthwise convolution and pointwise convolution, depthwise separable convolutions significantly reduce computational complexity and model parameters, making the model more lightweight and suitable for devices with limited computational resources. Within the strategy of multi-scale feature extraction, the application of 1 &#x00D7; 1, 3 &#x00D7; 3, and 5 &#x00D7; 5 convolutions caters to the necessity of discerning features across differing scales. 1 &#x00D7; 1 convolutions are primarily used to adjust the number of channels, reduce the number of parameters, while still retaining spatial features; 3 &#x00D7; 3 convolutions, a common convolution size, are effective in capturing local features; and 5 &#x00D7; 5 convolutions are capable of capturing features from a larger receptive field. However, the direct use of 5 &#x00D7; 5 convolutions involves a large number of parameters, so two stacked 3 &#x00D7; 3 convolutions are used as a substitute for a single 5 &#x00D7; 5 convolution, which not only expands the receptive field but also reduces the number of parameters, enhancing network efficiency. Finally, the integration of skip connections aims to mitigate the vanishing gradient issue encountered in training deep networks, which maintains network depth while facilitating effective information flow between layers. Skip connections allow for the direct transfer of information from shallower layers to deeper layers, aiding in the restoration of detail information and improving segmentation accuracy.</p>
<p>Overall, the MSBE takes into consideration multi-scale feature extraction, computational efficiency, and effective information flow. This approach is pivotal in refining the utilization of computational resources while maintaining the integrity of the model&#x2019;s performance, especially critical in the segmentation of high-resolution whole slide images. Next, we&#x2019;ll delve into Dimensionality Expansion, Depthwise Separable Convolution, Multi-Scale Feature Extraction, Dimensionality Reduction, and Skip Connections in detail.</p>
<p><bold>Dimensionality Expansion:</bold> The architecture employs a standard <inline-formula id="ieqn-1"><alternatives><inline-graphic xlink:href="ieqn-1.tif"/><tex-math id="tex-ieqn-1"><![CDATA[$1\times 1$]]></tex-math><mml:math id="mml-ieqn-1"></mml:math></alternatives></inline-formula> convolution to upscale the input feature map to <italic>D</italic> channels. Dimensionality expansion enhances the neural network&#x2019;s representational capability, providing the model with a larger parameter space to capture complex feature relationships, thereby reducing the bottleneck of information flow through the network. It increases the non-linearity of feature representation and supplies a richer informational basis for subsequent multi-scale feature extraction. This facilitation is crucial for the model&#x2019;s ability to more effectively learn and integrate diverse feature details.</p>
<p>For an input feature map <inline-formula id="ieqn-2"><alternatives><inline-graphic xlink:href="ieqn-2.tif"/><tex-math id="tex-ieqn-2"><![CDATA[$X\in \mathbb{R}^{H\times W\times C}$]]></tex-math><mml:math id="mml-ieqn-2"></mml:math></alternatives></inline-formula>, where <italic>H</italic> and <italic>W</italic> denote the height and width respectively, and C is the number of channels, the dimensionality expansion can be described using a <inline-formula id="ieqn-3"><alternatives><inline-graphic xlink:href="ieqn-3.tif"/><tex-math id="tex-ieqn-3"><![CDATA[$1\times 1$]]></tex-math><mml:math id="mml-ieqn-3"></mml:math></alternatives></inline-formula> convolution as follows:
<disp-formula id="eqn-1"><label>(1)</label><alternatives><graphic mimetype="image" mime-subtype="tif" xlink:href="eqn-1.tif"/><tex-math id="tex-eqn-1"><![CDATA[\begin{equation}X^{\prime}=X*K\end{equation}]]></tex-math><mml:math id="mml-eqn-1" display="block"></mml:math></alternatives></disp-formula>where <inline-formula id="ieqn-4"><alternatives><inline-graphic xlink:href="ieqn-4.tif"/><tex-math id="tex-ieqn-4"><![CDATA[$K\in \mathbb{R}^{1\times 1\times C\times D}$]]></tex-math><mml:math id="mml-ieqn-4"></mml:math></alternatives></inline-formula> represents the <inline-formula id="ieqn-5"><alternatives><inline-graphic xlink:href="ieqn-5.tif"/><tex-math id="tex-ieqn-5"><![CDATA[$1\times 1$]]></tex-math><mml:math id="mml-ieqn-5"></mml:math></alternatives></inline-formula> convolutional kernel, and the resulting feature map <italic>X&#x2019;</italic> possesses D channels.</p>
<p><bold>Depthwise Separable Convolution:</bold> This architecture comprehensively employs depthwise separable convolutions to reduce the number of parameters and computational costs while maintaining robust feature extraction capabilities. This makes the network more lightweight and ensures efficiency without compromising on powerful representational capacity. Depthwise separable convolution involves an initial depthwise convolution, followed by a pointwise convolution. For a <inline-formula id="ieqn-6"><alternatives><inline-graphic xlink:href="ieqn-6.tif"/><tex-math id="tex-ieqn-6"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-6"></mml:math></alternatives></inline-formula> depthwise separable convolution, this can be described as:
<disp-formula id="eqn-2"><label>(2)</label><alternatives><graphic mimetype="image" mime-subtype="tif" xlink:href="eqn-2.tif"/><tex-math id="tex-eqn-2"><![CDATA[\begin{equation}Y=(X^{\prime}\circledast K_{d})*K_{p}\end{equation}]]></tex-math><mml:math id="mml-eqn-2" display="block"></mml:math></alternatives></disp-formula>where <inline-formula id="ieqn-7"><alternatives><inline-graphic xlink:href="ieqn-7.tif"/><tex-math id="tex-ieqn-7"><![CDATA[$\circledast$]]></tex-math><mml:math id="mml-ieqn-7"></mml:math></alternatives></inline-formula> denotes the depthwise convolution operation, <inline-formula id="ieqn-8"><alternatives><inline-graphic xlink:href="ieqn-8.tif"/><tex-math id="tex-ieqn-8"><![CDATA[$K_{d}\in \mathbb{R}^{3\times 3\times D\times 1}$]]></tex-math><mml:math id="mml-ieqn-8"></mml:math></alternatives></inline-formula> is the depthwise convolutional kernel, and <inline-formula id="ieqn-9"><alternatives><inline-graphic xlink:href="ieqn-9.tif"/><tex-math id="tex-ieqn-9"><![CDATA[$K_{p}\in \mathbb{R}^{1\times 1\times D\times D}$]]></tex-math><mml:math id="mml-ieqn-9"></mml:math></alternatives></inline-formula> is the pointwise convolutional kernel.</p>
<p><bold>Multi-Scale Feature Extraction:</bold> In order to capture information of various scales within the image more effectively, a multi-scale feature extraction strategy is introduced. The encoder selects different convolutional kernel sizes based on distinct patterns:</p>
<p><bold>Mode &#x003D; 0:</bold> Employs both <inline-formula id="ieqn-10"><alternatives><inline-graphic xlink:href="ieqn-10.tif"/><tex-math id="tex-ieqn-10"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-10"></mml:math></alternatives></inline-formula> and a tandem of two <inline-formula id="ieqn-11"><alternatives><inline-graphic xlink:href="ieqn-11.tif"/><tex-math id="tex-ieqn-11"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-11"></mml:math></alternatives></inline-formula> kernels to simulate the effect of a <inline-formula id="ieqn-12"><alternatives><inline-graphic xlink:href="ieqn-12.tif"/><tex-math id="tex-ieqn-12"><![CDATA[$5\times 5$]]></tex-math><mml:math id="mml-ieqn-12"></mml:math></alternatives></inline-formula> kernel for multi-scale feature extraction, as illustrated in <xref ref-type="fig" rid="fig-3">Fig. 3a</xref>.</p>
<p><bold>Mode &#x003D; 1:</bold> Utilizes solely the <inline-formula id="ieqn-13"><alternatives><inline-graphic xlink:href="ieqn-13.tif"/><tex-math id="tex-ieqn-13"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-13"></mml:math></alternatives></inline-formula> kernel to extract features with a smaller receptive field, as depicted in <xref ref-type="fig" rid="fig-3">Fig. 3b</xref>.</p>
<p><bold>Mode &#x003D; 2:</bold> Engages two consecutive <inline-formula id="ieqn-14"><alternatives><inline-graphic xlink:href="ieqn-14.tif"/><tex-math id="tex-ieqn-14"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-14"></mml:math></alternatives></inline-formula> kernels to approximate the impact of a <inline-formula id="ieqn-15"><alternatives><inline-graphic xlink:href="ieqn-15.tif"/><tex-math id="tex-ieqn-15"><![CDATA[$5\times 5$]]></tex-math><mml:math id="mml-ieqn-15"></mml:math></alternatives></inline-formula> convolution, aiming to expand the receptive field and capture a broader range of contextual information, as shown in <xref ref-type="fig" rid="fig-3">Fig. 3c</xref>.</p>
<p>The representation of Mode 0, using a <inline-formula id="ieqn-16"><alternatives><inline-graphic xlink:href="ieqn-16.tif"/><tex-math id="tex-ieqn-16"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-16"></mml:math></alternatives></inline-formula> kernel and two <inline-formula id="ieqn-17"><alternatives><inline-graphic xlink:href="ieqn-17.tif"/><tex-math id="tex-ieqn-17"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-17"></mml:math></alternatives></inline-formula> kernels to simulate a <inline-formula id="ieqn-18"><alternatives><inline-graphic xlink:href="ieqn-18.tif"/><tex-math id="tex-ieqn-18"><![CDATA[$5\times 5$]]></tex-math><mml:math id="mml-ieqn-18"></mml:math></alternatives></inline-formula> convolutional effect, is as follows:
<disp-formula id="eqn-3"><label>(3)</label><alternatives><graphic mimetype="image" mime-subtype="tif" xlink:href="eqn-3.tif"/><tex-math id="tex-eqn-3"><![CDATA[\begin{equation}Z_{3\times 3}=X^{\prime}*K_{3\times 3}\end{equation}]]></tex-math><mml:math id="mml-eqn-3" display="block"></mml:math></alternatives></disp-formula></p>



<p><disp-formula id="eqn-4"><label>(4)</label><alternatives><graphic mimetype="image" mime-subtype="tif" xlink:href="eqn-4.tif"/><tex-math id="tex-eqn-4"><![CDATA[\begin{equation}Z_{5\times 5}=X^{\prime}*K_{3\times 3}*K_{3\times 3}\end{equation}]]></tex-math><mml:math id="mml-eqn-4" display="block"></mml:math></alternatives></disp-formula>where <inline-formula id="ieqn-19"><alternatives><inline-graphic xlink:href="ieqn-19.tif"/><tex-math id="tex-ieqn-19"><![CDATA[$K_{3\times 3}\in \mathbb{R}^{3\times 3\times D\times D}$]]></tex-math><mml:math id="mml-ieqn-19"></mml:math></alternatives></inline-formula> represents the <inline-formula id="ieqn-20"><alternatives><inline-graphic xlink:href="ieqn-20.tif"/><tex-math id="tex-ieqn-20"><![CDATA[$3\times 3$]]></tex-math><mml:math id="mml-ieqn-20"></mml:math></alternatives></inline-formula> convolutional kernel.</p>
<p><bold>Dimensionality Reduction:</bold> Following the extraction of features at different scales, a convolutional operation is applied to reduce the channel count of the output feature map, aligning it with the channel count of the input feature map in preparation for the skip connections. The dimensionality reduction can be represented using a <inline-formula id="ieqn-21"><alternatives><inline-graphic xlink:href="ieqn-21.tif"/><tex-math id="tex-ieqn-21"><![CDATA[$1\times 1$]]></tex-math><mml:math id="mml-ieqn-21"></mml:math></alternatives></inline-formula> convolution. For the concatenated feature map <inline-formula id="ieqn-22"><alternatives><inline-graphic xlink:href="ieqn-22.tif"/><tex-math id="tex-ieqn-22"><![CDATA[$Z\in \mathbb{R}^{H\times W\times 2D}$]]></tex-math><mml:math id="mml-ieqn-22"></mml:math></alternatives></inline-formula>, the dimensionality reduction operation can be expressed as:</p>
<p><disp-formula id="eqn-5"><label>(5)</label><alternatives><graphic mimetype="image" mime-subtype="tif" xlink:href="eqn-5.tif"/><tex-math id="tex-eqn-5"><![CDATA[\begin{equation}Z^{\prime}=Z*K_{red}\end{equation}]]></tex-math><mml:math id="mml-eqn-5" display="block"></mml:math></alternatives></disp-formula>where <inline-formula id="ieqn-23"><alternatives><inline-graphic xlink:href="ieqn-23.tif"/><tex-math id="tex-ieqn-23"><![CDATA[$K_{red}\in \mathbb{R}^{1\times 1\times 2D\times C}$]]></tex-math><mml:math id="mml-ieqn-23"></mml:math></alternatives></inline-formula> is the convolutional kernel used for dimensionality reduction.</p>
<p><bold>Skip Connections:</bold> The design of skip connections ensures improved information propagation through the depth of the model, guaranteeing that shallow features can be directly transmitted to deeper layers. This approach mitigates the common issue of gradient vanishing in deep networks. Additionally, skip connections facilitate easier network training and enable the construction of deeper network architectures, which, in turn, enhances the stability of model training.</p>
<p>The core concept of the MSB Encoder lies in the selective application of various convolutional kernel sizes based on different modes for feature extraction. Moreover, the encoder dynamically chooses the convolutional kernel size according to the mode, providing enhanced flexibility for feature extraction. The incorporation of skip connections enables the model to better maintain and convey information at deeper layers, thereby mitigating the issue of vanishing gradients and enhancing training stability. Compared to traditional convolutions, depthwise separable convolutions significantly reduce the number of parameters, thereby decreasing computational costs, but also retain impressive feature extraction capabilities. This design renders the network more lightweight while maintaining powerful representational abilities and ensuring greater efficiency. In summary, the MSB encoder not only accounts for feature extraction across varying scales but also effectively controls the number of parameters, making it a highly promising feature extractor.</p>
</sec>
<sec id="s3_2">
<label>3.2</label>
<title>Cross-Layer Connected Segmentation Network Framework</title>
<p>We proposed an innovative segmentation network structure, the CrossLinkNet Framework, which employs cross-layer connections to segment pathological images. The CrossLinkNet Framework&#x2019;s key advantage is its ability to progressively learn and integrate information through multiple sub-modules, thereby enhancing the model&#x2019;s feature extraction and contextual information recognition capability by focusing on extracting features across various dimensions. This method bolsters the model&#x2019;s robustness, augmenting its tolerance to noise, variations, and perturbations present in the input data. The CrossLinkNet Framework is highly adaptable, allowing for easy customization of encoder and decoder configurations to suit different tasks by modifying sub-module structures and parameters. Each sub-module can be replaced with high-performance encoders to handle different datasets and segmentation challenges. CrossLinkNet progressively learns and integrates features across layers, enhancing segmentation results. This flexibility renders the CrossLinkNet Framework suitable for a wide array of tumor segmentation tasks and scenarios.</p>
<p>The CrossLinkNet architecture is a novel image segmentation network structure, where the cornerstone is the implementation of cross-layer connections that facilitate information transfer between encoders and among decoders. This design optimizes the pathways of information flow, thereby enhancing segmentation accuracy. The network consists of two principal components: The encoders and the decoders, as illustrated in <xref ref-type="fig" rid="fig-4">Fig. 4</xref>.</p>
<p>The encoder, being the epicenter and novelty of the framework, aims to efficiently extract features from the input image. It employs a standard sequence of convolution, batch normalization, and activation functions to capture multi-level information from the input image, followed by max-pooling operations for downsampling. This creates feature maps at various depths, providing a rich source of information for the decoder. Diverging from conventional decoder designs, the decoder in CrossLinkNet is comprised of features from two upsampling branches and encoder features from the corresponding level. Subsequently, these upsampled feature maps are merged with the feature maps from the encoder through a concatenation operation, ensuring that cross-layer information is thoroughly harnessed during the decoding process. Such an arrangement allows the model to concurrently capture both local details and global contextual information of the image, significantly amplifying the precision of segmentation.</p>
<p>The encoder component of the CrossLinkNet framework is designed with remarkable flexibility, accommodating a multitude of feature extraction architectures. In this paper, the MSB encoder proposed in the preceding subsection is employed as the default encoder within the CrossLinkNet architecture, with the overall network architecture illustrated in <xref ref-type="fig" rid="fig-1">Fig. 1</xref>. Furthermore, CrossLinkNet strategically integrates batch normalization layers post multiple convolutional layers, which can expedite the convergence of the network during the training of deep learning models and also contribute to enhancing the generalization performance of the model. This design thoroughly contemplates the balance between model training efficiency and performance.</p>
<p>In summary, CrossLinkNet presents an image segmentation framework utilizing cross-layer connections to optimize pathways for information flow and enhance the feature representation capabilities of the model. The flexibility of its encoder design, the innovation within its decoder structure, and the incorporation of batch normalization collectively contribute to positioning CrossLinkNet as a highly accurate solution for image segmentation tasks. It possesses vast potential for application and demonstrates exceptional ability in feature extraction.</p>
</sec>
<sec id="s3_3">
<label>3.3</label>
<title>Interpretability in MSBE and CrossLinkNet</title>
<p>During the design of the CrossLinkNet framework and MSBE, we specifically focused on enhancing the interpretability of the model. This is particularly vital in medical image analysis, where understanding AI&#x2019;s decision-making process can significantly increase medical professionals&#x2019; trust in AI-assisted diagnostic systems. Here is how we enhanced the interpretability of CrossLinkNet and MSBE by designing specific model structural components.</p>
<p><bold>Multi-Scale Feature Extraction:</bold> MSBE processes images at multiple scales using different modes (Mode&#x003D;0, Mode&#x003D;1, Mode&#x003D;2), allowing the model to capture a range of features from fine textures to macro structures. This multi-level feature extraction enhances the model&#x2019;s performance and makes the decision-making process of the model more transparent. Visualizing the feature activation maps at different scales, medical practitioners can clearly see how the model makes diagnoses based on different details of the image, which offering a clear understanding about the decision-making logic of AI.</p>
<p><bold>Cross-Layer Connections:</bold> The cross-layer connection design in CrossLinkNet enables high-level features to be directly associated with low-level features, which not only aids in the propagation of gradients, reducing information loss during training but also enhances the model&#x2019;s interpretability. Through these connections, we can trace how high-level decisions are directly related to the original image, providing medical professionals with an intuitive way to verify the basis of the model&#x2019;s decisions.</p>
<p>Overall, the design of CrossLinkNet and MSBE has fully taken into account the importance of interpretability in medical image analysis, especially in critical application areas like cancer diagnosis. By combining high performance with high interpretability, our model is capable of providing accurate diagnostic results, and also allows doctors to understand and trust the AI&#x2019;s decision-making process, which is of significant importance for advancing the application of AI in the medical field.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Experiments Setting</title>
<p>This section is devoted to presenting the experimental results and analysis, conducting a thorough evaluation of the MSB encoder and the universal segmentation network CrossLinkNet using publicly available pathological image datasets. Initially, the experimental datasets and environment are introduced, followed by a detailed description of the evaluation metrics employed.</p>
<sec id="s4_1">
<label>4.1</label>
<title>Dataset</title>
<p>The dataset utilized in this chapter originates from the &#x201C;BOT Series Competition on Pathological Slide Recognition AI Challenge&#x201D;, focusing on a gastric cancer digital pathology image dataset. This dataset emphasizes the pathological characteristics of gastric cancer and is designed to support high-quality automated pathological image recognition. To ensure the dataset&#x2019;s quality and accuracy, during the initial phase, each image was meticulously annotated by a medically knowledgeable pathologist and subsequently subjected to careful review by an expert panel. In later stages, images were annotated by two pathologists with extensive knowledge in the field, with their work again reviewed by experts post-completion. Such a multi-stage, multi-reviewer process guarantees the accuracy of the annotations in the dataset and minimizes errors due to subjective judgment to the greatest extent possible.</p>
</sec>
<sec id="s4_2">
<label>4.2</label>
<title>Experiments Steps</title>
<p>In the experimental section of the paper, the annotation files in .svg format were initially converted into binary mask images in .png format. Subsequently, the pathological and mask images, at a resolution of 2048 &#x00D7; 2048, were scaled down to 256 &#x00D7; 256 pixels for storage. To validate the model&#x2019;s generalizability, the dataset was randomly split into a training set of 490 images, a validation set of 70 images, and a test set of 140 images. Each model was trained on the training set, validated on the validation set, and evaluated on the test set. The detailed distribution of the dataset is shown in <?A3B2 "tbl1",5,"anchor"?><xref ref-type="table" rid="table-1">Table 1</xref>.</p>
<table-wrap id="table-1">
<label>Table 1</label>
<caption>
<title>Dataset distribution for the BOT datasets</title>
</caption>
<alternatives>
<graphic mimetype="image" mime-subtype="tif" xlink:href="table-1.tif"/>
<table>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Dataset</th>
<th>Training</th>
<th>Validation</th>
<th>Testing</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>BOT Competiton</td>
<td>490</td>
<td>70</td>
<td>140</td>
<td>700</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The first part of the experiment compared our proposed MSB encoder with four classic encoder architectures: VGG, ResNet34, ResNet50, and MobileNet, to explore performance differences across various backbone networks. Following this, our newly designed CrossLinkNet was compared with existing architectures like UNet and SegNet under the same encoder configurations (namely, VGG, ResNet34, ResNet50, MobileNet, and MSBE). Subsequently, to comprehensively assess the superiority of the proposed model, we conducted extensive comparative experiments of CrossLinkNet (MSBE) with a series of classical networks, such as SegNet, UNet&#x002B;&#x002B;, as well as the renowned DeepLabV3&#x002B; and the recently introduced state-of-the-art model, PidNet. Finally, to confirm the applicability of the proposed universal segmentation network to other segmentation tasks, the CrossLinkNet (MSBE) network was also tested using the Kvasir gastrointestinal polyp segmentation dataset.</p>
<p>In summary, the efficacy of the MSB encoder, the performance of the universal segmentation network CrossLinkNet, the combined effect of CrossLinkNet (MSBE), and its performance on other datasets were evaluated through four different ablation studies. This multi-dimensional validation confirmed the high performance of the proposed model.</p>
</sec>
<sec id="s4_3">
<label>4.3</label>
<title>Experiments Environment</title>
<p>In this study, we conducted experimental validation on a high-end workstation with the following configuration: An Intel Core i7-8700k CPU, Nvidia GTX 3090 GPU, and 64 GB of system memory. The workstation was running the Ubuntu 20.04 LTS operating system. The experiments were performed using the deep learning framework Keras, with cross-entropy loss function as the foundation and stochastic gradient descent as the optimizer for the experimental model. The training was carried out for 100 epochs with a batch size set to 16. These settings enabled us to achieve favorable results within a relatively short time and allowed us to train and test more complex deep learning models.</p>
</sec>
<sec id="s4_4">
<label>4.4</label>
<title>Evaluation Metrics</title>
<p>In our experiments, the evaluation metrics include Frequency-Weighted Intersection over Union (FW_IoU), Mean Intersection over Union (mIoU), Mean Dice Coefficient (mDice), Pixel Accuracy (PA), Mean Average Precision (mAP), Mean Recall(mRecall). Through these metrics, a comprehensive understanding of the model&#x2019;s performance can be gained, facilitating model assessment, improvement, and optimization.</p>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>Experiments Result</title>
<sec id="s5_1">
<label>5.1</label>
<title>Comparison with Different Encoders</title>
<p>VGG, ResNet34, ResNet50, and MobileNet are classic convolutional neural network architectures often used as feature extractors to assist other primary networks in feature extraction. In our experimental design, we developed an innovative multi-scale, multi-branch feature encoder (MSBE) module and integrated it into various baseline networks such as UNet, SegNet, and the newly proposed CrossLinkNet. We evaluated the performance of the MSB encoder in image segmentation tasks through comparative experiments. The results are shown in <?A3B2 "tbl2",5,"anchor"?><xref ref-type="table" rid="table-2">Table 2</xref>. For the three sets of experimental results, we used different encoders and conducted ablation studies on each encoder using different backbone network structures to accurately assess their performance in image segmentation tasks.</p>
<table-wrap id="table-2">
<label>Table 2</label>
<caption>
<title>Ablation experiment of different feature encoder with same backbone networks</title>
</caption>
<alternatives>
<graphic mimetype="image" mime-subtype="tif" xlink:href="table-2.tif"/>
<table>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>ID</th>
<th>Model</th>
<th>Params (M)</th>
<th>GFLOPs</th>
<th>FW_IoU</th>
<th>mIoU</th>
<th>mDice</th>
<th>PA</th>
<th>mAP</th>
<th>mRecall</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="5">1</td>
<td>VGG-UNet</td>
<td>19.4</td>
<td>52.3</td>
<td>0.8580</td>
<td>0.7591</td>
<td>0.8552</td>
<td>0.9172</td>
<td>0.8372</td>
<td><bold>0.8772</bold></td>
</tr>
<tr>
<td>ResNet34-UNet</td>
<td>24.4</td>
<td>19.9</td>
<td>0.8491</td>
<td>0.7243</td>
<td>0.8268</td>
<td>0.9171</td>
<td>0.8790</td>
<td>0.7921</td>
</tr>
<tr>
<td>ResNet50-UNet</td>
<td>31.3</td>
<td>25.8</td>
<td>0.8683</td>
<td>0.7647</td>
<td>0.8582</td>
<td>0.9268</td>
<td>0.8752</td>
<td>0.8434</td>
</tr>
<tr>
<td>MobileNet-UNet</td>
<td>7.8</td>
<td><bold>13.6</bold></td>
<td>0.8767</td>
<td>0.7742</td>
<td>0.8645</td>
<td>0.9334</td>
<td>0.9063</td>
<td>0.8338</td>
</tr>
<tr>
<td>MSBE-UNet</td>
<td><bold>3.9</bold></td>
<td>26.6</td>
<td><bold>0.8824</bold></td>
<td><bold>0.7845</bold></td>
<td><bold>0.8718</bold></td>
<td><bold>0.9366</bold></td>
<td><bold>0.9112</bold></td>
<td>0.8423</td>
</tr>
<tr>
<td rowspan="5">2</td>
<td>VGG-SegNet</td>
<td>18.6</td>
<td>48.6</td>
<td>0.8573</td>
<td>0.7337</td>
<td>0.8335</td>
<td>0.9236</td>
<td>0.9146</td>
<td>0.7872</td>
</tr>
<tr>
<td>ResNet34-SegNet</td>
<td>24</td>
<td>17.5</td>
<td>0.8237</td>
<td>0.6855</td>
<td>0.7958</td>
<td>0.8994</td>
<td>0.8291</td>
<td>0.7716</td>
</tr>
<tr>
<td>ResNet50-SegNet</td>
<td>29.8</td>
<td>19.8</td>
<td>0.8316</td>
<td>0.6804</td>
<td>0.7886</td>
<td>0.9103</td>
<td>0.9222</td>
<td>0.7333</td>
</tr>
<tr>
<td>MobileNet-SegNet</td>
<td>7.1</td>
<td><bold>10.0</bold></td>
<td>0.8650</td>
<td>0.7474</td>
<td>0.8441</td>
<td>0.9282</td>
<td><bold>0.9250</bold></td>
<td>0.7971</td>
</tr>
<tr>
<td>MSBE-SegNet</td>
<td><bold>3.5</bold></td>
<td>25.5</td>
<td><bold>0.8784</bold></td>
<td><bold>0.7764</bold></td>
<td><bold>0.8660</bold></td>
<td><bold>0.9346</bold></td>
<td>0.9127</td>
<td><bold>0.8327</bold></td>
</tr>
<tr>
<td rowspan="5">3</td>
<td>VGG-CrossLinkNet</td>
<td>30.7</td>
<td>67.0</td>
<td>0.8710</td>
<td>0.7675</td>
<td>0.8600</td>
<td>0.9290</td>
<td>0.8852</td>
<td>0.8395</td>
</tr>
<tr>
<td>ResNet34-CrossLinkNet</td>
<td>35.4</td>
<td>33.0</td>
<td>0.8645</td>
<td>0.7561</td>
<td>0.8516</td>
<td>0.9250</td>
<td>0.8779</td>
<td>0.8304</td>
</tr>
<tr>
<td>ResNet50-CrossLinkNet</td>
<td>49.6</td>
<td>47.1</td>
<td>0.8719</td>
<td>0.7762</td>
<td>0.8668</td>
<td>0.9276</td>
<td>0.8634</td>
<td><bold>0.8704</bold></td>
</tr>
<tr>
<td>MobileNet-CrossLinkNet</td>
<td>21.3</td>
<td><bold>29.5</bold></td>
<td>0.8779</td>
<td>0.7780</td>
<td>0.8674</td>
<td>0.9337</td>
<td>0.9005</td>
<td>0.8416</td>
</tr>
<tr>
<td>MSBE-CrossLinkNet</td>
<td><bold>14.9</bold></td>
<td>45.9</td>
<td><bold>0.8829</bold></td>
<td><bold>0.7856</bold></td>
<td><bold>0.8727</bold></td>
<td><bold>0.9369</bold></td>
<td><bold>0.9113</bold></td>
<td>0.8436</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In the first set of experiments of this study, we chose UNet as the basic backbone network and combined it with five different encoder structures: VGG, ResNet34, ResNet50, MobileNet, and MSBE for ablation studies. The results showed that the UNet model, with the MSBE encoder, performed excellently on multiple performance metrics, including FW_IoU, mIoU, mDice, PA, and mAP. Specifically, MSBE-UNet achieved the best performance in terms of frequency weighted IoU, mean IoU, mean Dice coefficient, pixel accuracy, and mean precision. In terms of mIoU, MSBE-UNet was about 2.89% higher than the average of the other four encoder structures. In terms of mAP, its performance was 7.4%, 3.21%, 3.6%, and 0.48% higher than VGG, ResNet34, ResNet50, and MobileNet, respectively. However, in terms of mRecall, the performance of MSBE-UNet was slightly inferior to VGG-UNet and ResNet50-UNet. This result suggests that the MSBE encoder is more effective in feature extraction and preserving image details compared to other encoder structures.</p>
<p>In the second set of experiments of this study, SegNet was used as the base backbone network, and the MSBE encoder was compared with four different encoder structures: VGG, ResNet34, ResNet50, and MobileNet. The results indicated that the MSBE encoder integrated into the SegNet network exhibited superior performance compared to the other four encoders. Specifically, MSBE-SegNet surpassed the other combinations in all performance metrics except mAP. In terms of mIoU, MSBE-SegNet was 6.47% higher than the average of the other encoder structures. Also, in mRecall, MSBE-SegNet&#x2019;s performance was 4.55%, 6.1%, 9.94%, and 3.55% higher than VGG, ResNet34, ResNet50, and MobileNet, respectively. Although MSBE-SegNet was slightly inferior to MobileNet-SegNet in terms of mAP, it performed better in other key performance metrics, demonstrating the effectiveness of the MSBE encoder in the SegNet framework.</p>
<p>In the third set of experiments of this study, we utilized the newly proposed CrossLinkNet as the base backbone network and integrated five different encoder structures: VGG, ResNet34, ResNet50, MobileNet, and MSBE for ablation studies. The results showed that the CrossLinkNet network with the MSBE encoder exhibited excellent performance, outperforming the other four encoders. Specifically, MSBE-CrossLinkNet achieved optimal performance in all five metrics: FW_IoU, mIoU, mDice, PA, and mAP, with scores of 88.29%, 78.56%, 87.27%, 93.69%, and 91.13%, respectively. In terms of mRecall, MSBE-CrossLinkNet ranked second at 84.36%, slightly behind ResNet50-CrossLinkNet&#x2019;s 87.04%. These data indicate that in the CrossLinkNet-based experimental setup, MSBE-CrossLinkNet achieved the best performance in all metrics except mean recall, further confirming the superior performance of the MSBE encoder.</p>
<p>As demonstrated in <xref ref-type="table" rid="table-2">Table 2</xref>, with different backbone networks, the model parameter quantity (Params) is generally lower when employing MSBE as the feature encoder compared to other feature encoders. When combined with UNet, SegNet, and CrossLinkNet, the parameter quantities of MSBE are merely 3.9, 3.5 and 14.9M respectively, significantly lower than other encoders. This highlights the parameter efficiency advantage of MSBE. These results demonstrate that MSBE efficiently extracts features with a reduced parameter count, significantly lightening the model&#x2019;s storage and deployment load. Additionally, the GFLOPs of MSBE are not the lowest, but which remain at a moderate level. Specifically, in integration with CrossLinkNet, MSBE&#x2019;s GFLOPs are 45.9. Although this is higher than MobileNet-UNet&#x2019;s GFLOPs, it remains notably superior to the VGG and ResNet50 as encoders. MSBE achieves a reasonable computational complexity and excellent performance with fewer parameters, demonstrating its efficiency and balance between computational efficiency in feature extraction.</p>
<p>The comprehensive analysis of the three sets of experiments in this study shows that the MSBE encoder demonstrated the best or near-best performance in all three backbone network experiments (UNet, SegNet, and CrossLinkNet). The proposed MSBE encoder, with its theoretical multi-scale and multi-branch design advantages, has also been empirically confirmed to exhibit superior performance in segmentation tasks. Compared to traditional encoders like VGG, ResNet, and MobileNet, MSBE showed greater stability and performance across different backbone networks. This proves its vast potential for application in deep learning segmentation tasks. Subsequently, we&#x2019;ll discuss the theoretical foundation behind MSBE and the characteristics of its network structure, further analyzing the reasons for its effectiveness.</p>
<p>The design philosophy of MSBE is primarily based on two foundational principles: The importance of multi-scale features and the flexibility of the network&#x2019;s branching structure. In medical imaging, especially in the analysis of whole slide images, the size, shape, and texture of lesion areas can vary greatly. Conventional single-scale feature encoders are typically constrained by their fixed receptive field sizes, making it challenging to concurrently capture fine detail features and large-scale structural information. MSBE introduces multi-scale convolutional kernels, enabling the network to extract features at different scales and thus understand the image content more comprehensively. For instance, small-scale convolutional kernels can capture the fine details of an image, such as the edges and textures of cells, while large-scale kernels are able to extract broader contextual information, aiding in the understanding of the overall morphology and relative position of lesion areas. This multi-scale feature extraction strategy allows MSBE to more accurately identify and locate key areas within images. Furthermore, the multi-branch structure of MSBE enhances the network&#x2019;s flexibility and adaptiveness. Deviating from the conventional linear or singular pathway structures, MSBE&#x2019;s parallel branching enables the concurrent processing and integration of features across various scales. This design not only increases the network&#x2019;s capacity but also enables MSBE to better utilize the multi-level information in images, while concurrently reducing the parameter overhead. In experiments, these features enable MSBE to effectively improve the accuracy and robustness of segmentation tasks without significantly increasing computational complexity.</p>
</sec>
<sec id="s5_2">
<label>5.2</label>
<title>Comparison with Different Segmentation Network Framework</title>
<p>UNet and SegNet are established baseline models in computer vision, with UNet being prevalently utilized in medical image segmentation and SegNet being notable for semantic segmentation tasks. In our research, a novel network termed CrossLinkNet was developed for comparative analysis with these baseline models. This comparative study, detailed in <?A3B2 "tbl3",5,"anchor"?><xref ref-type="table" rid="table-3">Table 3</xref>, employed ablation experiments using a consistent set of encoders (VGG, ResNet34, ResNet50, MobileNet, and MSBE) across different backbone network structures. This approach was adopted to rigorously assess the efficacy of CrossLinkNet in image segmentation tasks. The following content will provide an in-depth analysis of the results from these five distinct experimental sets.</p>
<table-wrap id="table-3">
<label>Table 3</label>
<caption>
<title>Ablation experiment of different backbone networks with same feature encoder</title>
</caption>
<alternatives>
<graphic mimetype="image" mime-subtype="tif" xlink:href="table-3.tif"/>
<table>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>ID</th>
<th>Model</th>
<th>Params (M)</th>
<th>GFLOPs</th>
<th>FW_IoU</th>
<th>mIoU</th>
<th>mDice</th>
<th>PA</th>
<th>mAP</th>
<th>mRecall</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="3">1</td>
<td>VGG-UNet</td>
<td>19.4</td>
<td>52.3</td>
<td>0.8580</td>
<td>0.7591</td>
<td>0.8552</td>
<td>0.9172</td>
<td>0.8372</td>
<td><bold>0.8772</bold></td>
</tr>
<tr>
<td>VGG-SegNet</td>
<td><bold>18.6</bold></td>
<td><bold>48.6</bold></td>
<td>0.8573</td>
<td>0.7337</td>
<td>0.8335</td>
<td>0.9236</td>
<td><bold>0.9146</bold></td>
<td>0.7872</td>
</tr>
<tr>
<td>VGG-CrossLinkNet</td>
<td>30.7</td>
<td>67.0</td>
<td><bold>0.8710</bold></td>
<td><bold>0.7675</bold></td>
<td><bold>0.8600</bold></td>
<td><bold>0.9290</bold></td>
<td>0.8852</td>
<td>0.8395</td>
</tr>
<tr>
<td rowspan="3">2</td>
<td>ResNet34-UNet</td>
<td>24.4</td>
<td>19.9</td>
<td>0.8491</td>
<td>0.7243</td>
<td>0.8268</td>
<td>0.9171</td>
<td><bold>0.8790</bold></td>
<td>0.7921</td>
</tr>
<tr>
<td>ResNet34-SegNet</td>
<td><bold>24.0</bold></td>
<td><bold>17.5</bold></td>
<td>0.8237</td>
<td>0.6855</td>
<td>0.7958</td>
<td>0.8994</td>
<td>0.8291</td>
<td>0.7716</td>
</tr>
<tr>
<td>ResNet34-CrossLinkNet</td>
<td>35.4</td>
<td>33.0</td>
<td><bold>0.8645</bold></td>
<td><bold>0.7561</bold></td>
<td><bold>0.8516</bold></td>
<td><bold>0.9250</bold></td>
<td>0.8779</td>
<td><bold>0.8304</bold></td>
</tr>
<tr>
<td rowspan="3">3</td>
<td>ResNet50-UNet</td>
<td>31.3</td>
<td>25.8</td>
<td>0.8683</td>
<td>0.7647</td>
<td>0.8582</td>
<td>0.9268</td>
<td>0.8752</td>
<td>0.8434</td>
</tr>
<tr>
<td>ResNet50-SegNet</td>
<td><bold>29.8</bold></td>
<td><bold>19.8</bold></td>
<td>0.8316</td>
<td>0.6804</td>
<td>0.7886</td>
<td>0.9103</td>
<td><bold>0.9222</bold></td>
<td>0.7333</td>
</tr>
<tr>
<td>ResNet50-CrossLinkNet</td>
<td>49.6</td>
<td>47.1</td>
<td><bold>0.8719</bold></td>
<td><bold>0.7762</bold></td>
<td><bold>0.8668</bold></td>
<td><bold>0.9276</bold></td>
<td>0.8634</td>
<td><bold>0.8704</bold></td>
</tr>
<tr>
<td rowspan="3">4</td>
<td>MobileNet-UNet</td>
<td>7.8</td>
<td>13.6</td>
<td>0.8767</td>
<td>0.7742</td>
<td>0.8645</td>
<td>0.9334</td>
<td>0.9063</td>
<td>0.8338</td>
</tr>
<tr>
<td>MobileNet-SegNet</td>
<td><bold>7.1</bold></td>
<td><bold>10.0</bold></td>
<td>0.8650</td>
<td>0.7474</td>
<td>0.8441</td>
<td>0.9282</td>
<td><bold>0.9250</bold></td>
<td>0.7971</td>
</tr>
<tr>
<td>MobileNet-CrossLinkNet</td>
<td>21.3</td>
<td>29.5</td>
<td><bold>0.8779</bold></td>
<td><bold>0.7780</bold></td>
<td><bold>0.8674</bold></td>
<td><bold>0.9337</bold></td>
<td>0.9005</td>
<td><bold>0.8416</bold></td>
</tr>
<tr>
<td rowspan="3">5</td>
<td>MSBE-UNet</td>
<td>3.9</td>
<td>26.6</td>
<td>0.8824</td>
<td>0.7845</td>
<td>0.8718</td>
<td>0.9366</td>
<td>0.9112</td>
<td>0.8423</td>
</tr>
<tr>
<td>MSBE-SegNet</td>
<td><bold>3.5</bold></td>
<td><bold>25.5</bold></td>
<td>0.8784</td>
<td>0.7764</td>
<td>0.8660</td>
<td>0.9346</td>
<td><bold>0.9127</bold></td>
<td>0.8327</td>
</tr>
<tr>
<td>MSBE-CrossLinkNet</td>
<td>14.9</td>
<td>45.9</td>
<td><bold>0.8829</bold></td>
<td><bold>0.7856</bold></td>
<td><bold>0.8727</bold></td>
<td><bold>0.9369</bold></td>
<td>0.9113</td>
<td><bold>0.8436</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In the first set of experiments, VGG was utilized as the feature encoder, and ablation studies were conducted using three different backbone networks: UNet, SegNet, and CrossLinkNet. The results indicated that VGG-CrossLinkNet surpassed the other two comparative models across four metrics: FW_IoU, mIoU, mDice, and PA. Specifically, compared to VGG-UNet, CrossLinkNet showed an approximate 1.3% improvement in FW_IoU, about 0.85% in mIoU, around 0.5% in mDice, and a 1.2% increase in PA. Although VGG-SegNet achieved the highest score in mAP at 91.46%, its performance was poorer in other metrics. It is noteworthy that, while the mRecall of CrossLinkNet was slightly lower than that of VGG-UNet, it still represented an improvement of over 5% compared to VGG-SegNet. Overall, the performance of VGG-CrossLinkNet was superior to both VGG-UNet and VGG-SegNet.</p>
<p>In the second set of experiments, ResNet34 was employed as the feature encoder, and ablation studies were again conducted using the three distinct backbone networks: UNet, SegNet, and CrossLinkNet. The results demonstrated that ResNet34-CrossLinkNet achieved the best outcomes across five metrics: FW_IoU, mIoU, mDice, PA, and mRecall. Particularly in comparison with ResNet34-SegNet, ResNet34-CrossLinkNet showed superior performance in all metrics, with respective improvements of approximately 4.1%, 7.1%, 5.6%, 2.6%, 4.9%, and 5.9% in FW_IoU, mIoU, mDice, PA, mAP, and mRecall. When compared to ResNet34-UNet, ResNet34-CrossLinkNet was only marginally lower, by about 0.1%, in the mAP metric.</p>
<p>In the third set of experiments, ResNet50 was used as the feature encoder, with UNet, SegNet, and CrossLinkNet once again serving as the different backbone networks for ablation studies. The results indicated that ResNet50-CrossLinkNet achieved the best performance in five metrics: FW_IoU, mIoU, mDice, AP, and mAP. Notably, compared to ResNet50-SegNet, ResNet50-CrossLinkNet exhibited substantial improvements in these metrics, with increases of approximately 4%, 9.6%, 7.8%, 1.7%, and 13.7% in FW_IoU, mIoU, mDice, PA, and mRecall, respectively. Although ResNet50-UNet and ResNet50-SegNet showed slightly higher mean precision than ResNet50-CrossLinkNet, they were significantly outperformed in all other metrics by ResNet50-CrossLinkNet.</p>
<p>In the fourth set of experiments, MobileNet was employed as the feature encoder, with UNet, SegNet, and CrossLinkNet again used as different backbone networks for ablation studies. The results showed that MobileNet-CrossLinkNet achieved the best outcomes in five metrics: FW_IoU, mIoU, mDice, PA, and mRecall, with respective values of 87.79%, 77.8%, 86.74%, 93.37%, and 84.16%. Particularly in comparison with MobileNet-SegNet, MobileNet-CrossLinkNet demonstrated superior performance in these metrics, showing respective improvements of approximately 1.3%, 3.1%, 2.3%, 0.6%, and 4.5% in FW_IoU, mIoU, mDice, PA, and mRecall. Compared to MobileNet-UNet, MobileNet-CrossLinkNet was only marginally lower by 0.6 percentage points in mAP, but outperformed in all other evaluation metrics.</p>
<p>In the fifth set of experiments, MSBE was utilized as the feature encoder, with UNet, SegNet, and CrossLinkNet once again serving as the different backbone networks for ablation studies. The results indicated that MSBE-CrossLinkNet achieved the best performance across five metrics: FW_IoU, mIoU, mDice, PA, and mRecall, with respective values of 88.29%, 78.56%, 87.27%, 93.69%, and 84.36%. Notably, when compared to MSBE-UNet, MSBE-CrossLinkNet surpassed MSBE-UNet&#x2019;s metrics of 88.24%, 78.45%, 87.18%, 93.66%, 91.12%, and 84.23% in all evaluation metric. Compared to MSBE-SegNet, MSBE-CrossLinkNet was only marginally lower by 0.14 percentage points in mAP, yet excelled in all other evaluation metrics.</p>
<p>CrossLinkNet&#x2019;s foundational design is centered on boosting model performance via cross-layer connections, notably in feature fusion and information flow. This strategy, however, requires additional parameters and computations to realize such performance improvement, leading to a rise in both parameters and GFLOPs. Therefore, this can be regarded as a trade-off between model performance and computational resources.</p>
<p>Comparing the results from the above experiments, our proposed cross-layer linking segmentation network, CrossLinkNet, demonstrated exceptional performance under various encoders, particularly in key metrics like FW_IoU, mIoU, mDice, and PA, where it consistently showed significant advantages. Even in certain evaluation metrics (e.g., mAP) where other models may have a slight edge, CrossLinkNet still maintained a high level of overall performance, illustrating its stability and reliability in different environments. The following content will explore the network structural features of CrossLinkNet from a theoretical perspective, further analyzing the underpinnings for its effectiveness.</p>
<p>The primary hallmark of CrossLinkNet is its innovative cross-layer connection structure. CrossLinkNet establishes cross-layer connections among multiple levels, which effectively facilitates the fusion of low-level features with high-level features. This design enables the network to utilize deep abstract information for accurate region determination and to retain more detail information, thereby enhancing the precision of segmentation and the clarity of edges. Additionally, the cross-layer connections aid in better gradient backpropagation, minimizing the loss of information during training and thereby improving the network&#x2019;s learning efficiency and stability. Furthermore, apart from cross-layer connections, CrossLinkNet adopts a modular framework. This modular architecture is crucial for a deeper analysis and comprehension of the model&#x2019;s decision-making processes. Combining cross-layer connections with multi-scale feature encoding, CrossLinkNet not only excels in performance but also enhances the transparency of its network structure. In the aforementioned experiments, these advantages of CrossLinkNet enabled it to achieve superior performance.</p>
</sec>
<sec id="s5_3">
<label>5.3</label>
<title>Comparison with State-of-the-Arts Models on BOT Dataset</title>
<p>To validate the performance advantages of the proposed segmentation network, CrossLinkNet (MSBE), the experiment incorporated a series of classic network models for comparison, including SegNet, UNet&#x002B;&#x002B;, DeepLabV3&#x002B;, PidNet, and the latest state-of-the-art (SOTA) model, EnsNet. The experimental results, as shown in <?A3B2 "tbl4",5,"anchor"?><xref ref-type="table" rid="table-4">Table 4</xref>, clearly demonstrate the exceptional performance of CrossLinkNet (MSBE) across various evaluation metrics. Specifically, CrossLinkNet (MSBE) achieved an FW_IoU of 88.29%, significantly surpassing SegNet, UNet&#x002B;&#x002B;, DeepLabV3&#x002B;, PidNet, and EnsNet, and leading the second-best model, PidNet, by a margin of 0.35 percentage points. In terms of mIoU, its performance of 78.56% was also the highest, exceeding the second-placed PidNet by 0.98 percentage points. For the mDice, CrossLinkNet (MSBE) led other networks with a score of 87.27%. In PA, it reached the peak performance of 93.69%, slightly ahead of PidNet&#x2019;s 93.56% by 0.13 percentage points. Although in mAP, CrossLinkNet (MSBE)&#x2019;s 91.13% was slightly lower than PidNet&#x2019;s 91.96%, its performance was still highly competitive compared to other networks. Finally, CrossLinkNet (MSBE) also achieved the best result in mRecall with 84.36%.</p>
<table-wrap id="table-4">
<label>Table 4</label>
<caption>
<title>CrossLinkNet (MSBE) performance comparison to SOTA models with BOT datasets</title>
</caption>
<alternatives>
<graphic mimetype="image" mime-subtype="tif" xlink:href="table-4.tif"/>
<table>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Model</th>
<th>FW_IoU</th>
<th>mIoU</th>
<th>mDice</th>
<th>PA</th>
<th>mAP</th>
<th>mRecall</th>
</tr>
</thead>
<tbody>
<tr>
<td>SegNet [<xref ref-type="bibr" rid="ref-25">25</xref>]</td>
<td>0.8191</td>
<td>0.6668</td>
<td>0.7779</td>
<td>0.8998</td>
<td>0.8575</td>
<td>0.7363</td>
</tr>
<tr>
<td>UNet&#x002B;&#x002B; [<xref ref-type="bibr" rid="ref-26">26</xref>]</td>
<td>0.8554</td>
<td>0.7379</td>
<td>0.8377</td>
<td>0.9201</td>
<td>0.8761</td>
<td>0.8095</td>
</tr>
<tr>
<td>DeepLabV3&#x002B; [<xref ref-type="bibr" rid="ref-27">27</xref>]</td>
<td>0.8656</td>
<td>0.7544</td>
<td>0.8501</td>
<td>0.9263</td>
<td>0.8875</td>
<td>0.8221</td>
</tr>
<tr>
<td>PidNet [<xref ref-type="bibr" rid="ref-28">28</xref>]</td>
<td>0.8794</td>
<td>0.7758</td>
<td>0.8655</td>
<td>0.9356</td>
<td><bold>0.9196</bold></td>
<td>0.8284</td>
</tr>
<tr>
<td>EnsNet [<xref ref-type="bibr" rid="ref-29">29</xref>]</td>
<td>0.8694</td>
<td>0.7630</td>
<td>0.8566</td>
<td>0.9285</td>
<td>0.8903</td>
<td>0.8307</td>
</tr>
<tr>
<td>CrossLinkNet (MSBE)</td>
<td><bold>0.8829</bold></td>
<td><bold>0.7856</bold></td>
<td><bold>0.8727</bold></td>
<td><bold>0.9369</bold></td>
<td>0.9113</td>
<td><bold>0.8436</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>To visually demonstrate the performance of CrossLinkNet (MSBE) in image segmentation tasks, we selected three pathological images from the test set for visualization analysis, as shown in <?A3B2 "fig5",5,"anchor"?><xref ref-type="fig" rid="fig-5">Fig. 5</xref>. The first image contains multiple small tumor regions, used to assess the model&#x2019;s capability in segmenting small targets. The second image displays a large tumor area, aimed at examining the model&#x2019;s effectiveness in handling large targets. The third image is a mixed case, containing both large and small tumor regions, to evaluate the model&#x2019;s comprehensive segmentation performance for targets of varying sizes. In these images, the green areas represent the correctly predicted parts by the model (i.e., True Positive), the red areas indicate incorrect predictions (i.e., False Positive), and the blue areas denote the portions that the model failed to recognize (i.e., False Negative). The comparison of these three groups of images clearly demonstrates CrossLinkNet&#x2019;s exceptional segmentation performance in various scenarios.</p>
<fig id="fig-5">
<label>Figure 5</label>
<caption>
<title>Comparison results of pathological image segmentation experiment</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-5.tif"/>
</fig>
</sec>
<sec id="s5_4">
<label>5.4</label>
<title>Comparison with State-of-the-Arts Models on Kvasir Dataset</title>
<p>To assess the versatility of the proposed segmentation network, CrossLinkNet (MSBE), across various medical datasets, we conducted a series of comparative tests using the Kvasir dataset. The Kvasir dataset is specifically designed for research in gastrointestinal (GI) imagery and videos, encompassing various GI anatomical landmarks as well as a subset dedicated to GI polyp segmentation. This experiment utilized the polyp segmentation subset of the Kvasir dataset to evaluate the performance of CrossLinkNet (MSBE) in segmenting GI tract polyps. The comparative networks used in the experiment included SegNet, UNet&#x002B;&#x002B;, DeepLabV3&#x002B;, PidNet and EnsNet, with detailed results presented in <?A3B2 "tbl5",5,"anchor"?><xref ref-type="table" rid="table-5">Table 5</xref>.</p>
<table-wrap id="table-5">
<label>Table 5</label>
<caption>
<title>CrossLinkNet (MSBE) performance comparison to SOTA models with Kvasir datasets</title>
</caption>
<alternatives>
<graphic mimetype="image" mime-subtype="tif" xlink:href="table-5.tif"/>
<table>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Model</th>
<th>FW_IoU</th>
<th>mIoU</th>
<th>mDice</th>
<th>PA</th>
<th>mAP</th>
<th>mRecall</th>
</tr>
</thead>
<tbody>
<tr>
<td>SegNet [<xref ref-type="bibr" rid="ref-25">25</xref>]</td>
<td>0.9179</td>
<td>0.7943</td>
<td>0.8766</td>
<td>0.9562</td>
<td>0.9198</td>
<td>0.8434</td>
</tr>
<tr>
<td>UNet&#x002B;&#x002B; [<xref ref-type="bibr" rid="ref-26">26</xref>]</td>
<td>0.9395</td>
<td>0.8505</td>
<td>0.9151</td>
<td>0.9679</td>
<td>0.9307</td>
<td>0.9009</td>
</tr>
<tr>
<td>DeepLabV3&#x002B; [<xref ref-type="bibr" rid="ref-27">27</xref>]</td>
<td>0.9456</td>
<td>0.8670</td>
<td>0.9256</td>
<td>0.9708</td>
<td>0.9258</td>
<td>0.9254</td>
</tr>
<tr>
<td>PidNet [<xref ref-type="bibr" rid="ref-28">28</xref>]</td>
<td>0.9489</td>
<td>0.8734</td>
<td>0.9296</td>
<td>0.9728</td>
<td>0.9343</td>
<td>0.9250</td>
</tr>
<tr>
<td>EnsNet [<xref ref-type="bibr" rid="ref-29">29</xref>]</td>
<td>0.9523</td>
<td>0.8811</td>
<td>0.9343</td>
<td>0.9749</td>
<td>0.9435</td>
<td>0.9256</td>
</tr>
<tr>
<td>CrossLinkNet (MSBE)</td>
<td><bold>0.9537</bold></td>
<td><bold>0.8846</bold></td>
<td><bold>0.9364</bold></td>
<td><bold>0.9756</bold></td>
<td><bold>0.9440</bold></td>
<td><bold>0.9292</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Based on the experimental results from the Kvasir dataset, it is evident that our proposed CrossLinkNet (MSBE) outperformed all other comparative network models in every evaluation metric. Specifically, CrossLinkNet (MSBE) achieved a FW_IoU of 95.37%, which is 0.14 percentage points higher than the second-ranked EnsNet and 3.58 percentage points above the lowest-ranked SegNet. In the mIoU metric, CrossLinkNet (MSBE) led with 88.46%, ahead of EnsNet&#x2019;s 88.11% and surpassing SegNet&#x2019;s 79.43% by 9.03 percentage points. For mDice, CrossLinkNet (MSBE) scored 93.64%, slightly higher than EnsNet&#x2019;s 93.43% and more than 1 percentage point higher compared to SegNet, UNet&#x002B;&#x002B;, and DeepLabV3&#x002B;. In terms of PA, mAP, and mRecall, CrossLinkNet (MSBE) performed at 97.56%, 94.40%, and 92.92%, respectively, outperforming the second-best EnsNet by 0.07 percentage points, 0.05 percentage points, and 0.36 percentage points, while significantly surpassing the results of SegNet, UNet&#x002B;&#x002B;, and DeepLabV3&#x002B;.</p>
<p>To visually demonstrate the performance of the proposed CrossLinkNet model in different types of gastrointestinal (GI) tract image segmentation tasks, the experiment selected three representative images from the test set for visualization analysis, as illustrated in <?A3B2 "fig6",5,"anchor"?><xref ref-type="fig" rid="fig-6">Fig. 6</xref>. The first image displays a GI tract image containing small polyps, used to assess the model&#x2019;s segmentation effectiveness on small targets. The second image includes larger polyps, testing the model&#x2019;s ability to segment larger targets. The third image is a mixed case, containing both large and small polyps, used to evaluate the model&#x2019;s segmentation performance when dealing with targets of varying sizes simultaneously. In these images, green areas represent correctly predicted parts by the model (i.e., True Positive), red areas denote incorrect predictions (i.e., False Positive), and purple areas are the portions that the model failed to recognize (i.e., False Negative). The comparison of these three groups of images clearly indicates that CrossLinkNet demonstrates exceptional segmentation performance in various scenarios, validating its effectiveness and precision in medical image segmentation.</p>
<fig id="fig-6">
<label>Figure 6</label>
<caption>
<title>Comparison results of upper gastrointestinal polyp segmentation experiment</title>
</caption>
<graphic mimetype="image" mime-subtype="tif" xlink:href="CMC_49791-fig-6.tif"/>
</fig>
<p>In <xref ref-type="sec" rid="s5_3">Sections 5.3</xref> and <xref ref-type="sec" rid="s5_4">5.4</xref>, we have showcased the comparative results of CrossLinkNet (MSBE) against other models on the BOT and Kvasir datasets. These experimental results demonstrate that CrossLinkNet (MSBE) surpasses other network models on several critical performance metrics. The subsequent content will analyze the reasons for the superior performance of CrossLinkNet (MSBE) from a theoretical perspective.</p>
<p>The superior performance of CrossLinkNet (MSBE) across diverse datasets and tasks can be attributed to a confluence of critical factors. Foremost among these is MSBE&#x2019;s prowess in multi-scale feature extraction. By leveraging convolutional kernels of different scales, MSBE is capable of capturing both the detailed information and the global context of images simultaneously. This multi-scale feature extraction mechanism is particularly suitable for medical images, as they often contain lesions of varying significantly in size and morphology. Such a design enables CrossLinkNet (MSBE) to more accurately identify and segment lesion areas of various scales. Secondly, it benefits from the cross-layer connections and shortcut connections in CrossLinkNet (MSBE). The cross-layer connection structure in CrossLinkNet facilitates the fusion of features from different levels, integrating low-level details into the overarching decision-making process. This not only enhances the model&#x2019;s segmentation precision but also improves the delineation of edges, which is pivotal for intricate medical image segmentation tasks. Additionally, shortcut connections tackle the gradient vanishing issue inherent in deep network training, facilitating the seamless flow of information across layers. This process is instrumental in recapturing fine details and elevating segmentation fidelity. Finally, the modular design of CrossLinkNet (MSBE) plays a pivotal role. This design approach bolsters the model&#x2019;s transparency and explicability. Through cross-layer connections and multi-scale feature encoding, CrossLinkNet (MSBE) not only achieves excellent performance but also provides more comprehensible visual insights into its decision-making mechanics, which is of great significance for improving the accuracy and credibility of clinical diagnoses.</p>
<p>In conclusion, CrossLinkNet (MSBE)&#x2019;s distinguished performance in various tasks stems from its sophisticated multi-scale feature extraction, potent feature fusion mechanisms, and a strong focus on model interpretability. These integrated features not only elevate segmentation precision and robustness but also offer an interpretable solution for medical image segmentation tasks.</p>
</sec>
</sec>
<sec id="s6">
<label>6</label>
<title>Discussion</title>
<p>The introduction of CrossLinkNet, with its innovative MSBE and cross-layer connections, marks a significant advancement in whole-slide image segmentation. However, its performance is limited by the inherent variability and complexity of medical datasets, thus it is necessary to further explore adaptive feature extraction methods and data augmentation strategies to enhance robustness. Additionally, the model&#x2019;s computational intensity, necessitated by the high-resolution nature of whole-slide images, poses scalability challenges, especially in resource-constrained settings. Addressing this issue may require architectural optimizations to improve efficiency without compromising performance. Moreover, the impact of different feature encoders on the model&#x2019;s effectiveness is a crucial consideration. The comparative performance of MSBE against traditional encoders like VGG, ResNet, and MobileNet within the CrossLinkNet framework warrants a detailed examination to identify the most suitable encoder for specific segmentation tasks. Future enhancements to CrossLinkNet should focus on these areas to bolster its adaptability, scalability, and overall performance in the evolving field of medical images.</p>
</sec>
<sec id="s7">
<label>7</label>
<title>Conclusion</title>
<p>In this study, we present CrossLinkNet, an innovative segmentation network for whole slide images with a focus on explainable AI, marking a significant advancement in the field of medical image analysis. The introduction of the Multi-Scale Multi-Branch Feature Encoder (MSBE) represents a pivotal innovation, enhancing the network&#x2019;s ability to extract nuanced features effectively. This encoder&#x2019;s adaptability in configuration through hyperparameter adjustments allows for flexibility and precision in feature extraction. Furthermore, CrossLinkNet&#x2019;s unique cross-layer encoder-decoder connections markedly improve feature integration, thereby elevating segmentation accuracy. Our comprehensive experiments on datasets like BOT and Kvasir have showcased CrossLinkNet&#x2019;s exceptional performance. Crucially, the incorporation of explainable AI principles within CrossLinkNet addresses the pressing need for interpretability in medical diagnostics, thereby contributing to the field of digital pathology and automated diagnostic technologies. This research not only presents innovative methodologies but also emphasizes the pivotal role of explainable AI in advancing medical imaging analysis.</p>
</sec>
</body>
<back>
<ack>
<p>The authors would like to extend their gratitude to the Network and Data Security Key Laboratory of Sichuan Province for their invaluable support. We also acknowledge the significant contributions and resources provided by the University of Electronic Science and Technology of China. Their assistance was instrumental in the completion of this research.</p>
</ack>
<fn-group>
<fn fn-type="other">
<p><bold>Funding Statement:</bold> This work was supported by the National Natural Science Foundation of China (Grant Numbers: 62372083, 62072074, 62076054, 62027827, 62002047); the Sichuan Provincial Science and Technology Innovation Platform and Talent Program (Grant Number: 2022JDJQ0039); the Sichuan Provincial Science and Technology Support Program (Grant Numbers: 2022YFQ0045, 2022YFS0220, 2021YFG0131, 2023YFS0020, 2023YFS0197, 2023YFG0148); and the CCF-Baidu Open Fund (Grant Number: 202312).</p>
</fn>
<fn fn-type="other">
<p><bold>Author Contributions:</bold> The authors confirm contribution to the paper as follows: Study conception and design: Peng Xiao, Qi Zhong; dataset and experiments: Peng Xiao, Dongyuan Wu; analysis and interpretation of results: Peng Xiao, Zhen Qin; draft manuscript preparation: Peng Xiao, Jingxue Chen, Erqiang Zhou. All authors reviewed the results and approved the final version of the manuscript.</p>
</fn>
<fn fn-type="other">
<p><bold>Availability of Data and Materials:</bold> There are no data and materials available to share.</p>
</fn>
<fn fn-type="conflict">
<p><bold>Conflicts of Interest:</bold> The authors declare that they have no conflicts of interest to report regarding the present study.</p>
</fn>
</fn-group>
<ref-list content-type="authoryear">
<title>References</title>
<ref id="ref-1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Shafi</surname></string-name> and <string-name><given-names>A. V.</given-names> <surname>Parwani</surname></string-name></person-group>, &#x201C;<article-title>Artificial intelligence in diagnostic pathology</article-title>,&#x201D; <source>Diagn. Pathol.</source>, vol. <volume>18</volume>, no. <issue>1</issue>, pp. <fpage>109</fpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13000-023-01375-z">10.1186/s13000-023-01375-z</ext-link>.</mixed-citation></ref>
<ref id="ref-2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Hunter</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Hindocha</surname></string-name>, and <string-name><given-names>R. W.</given-names> <surname>Lee</surname></string-name></person-group>, &#x201C;<article-title>The role of artificial intelligence in early cancer diagnosis</article-title>,&#x201D; <source>Cancers</source>, vol. <volume>14</volume>, no. <issue>6</issue>, pp. <fpage>1524</fpage>, <year>2022</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3390/cancers14061524">10.3390/cancers14061524</ext-link>.</mixed-citation></ref>
<ref id="ref-3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B.</given-names> <surname>Lai</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Fu</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Jiang</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Peng</surname></string-name></person-group>, &#x201C;<article-title>Artificial intelligence in cancer pathology: Challenge to meet increasing demands of precision medicine</article-title>,&#x201D; <source>Int. J. Oncol.</source>, vol. <volume>63</volume>, no. <issue>3</issue>, pp. <fpage>1</fpage>&#x2013;<lpage>30</lpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3892/ijo.2023.5555">10.3892/ijo.2023.5555</ext-link>.</mixed-citation></ref>
<ref id="ref-4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Tondi</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Barni</surname></string-name></person-group>, &#x201C;<article-title>Universal detection of backdoor attacks via density-based clustering and centroids analysis</article-title>,&#x201D; <source>IEEE Trans. Inf. Forensics Secur.</source>, vol. <volume>19</volume>, no. <issue>7</issue>, pp. <fpage>970</fpage>&#x2013;<lpage>984</lpage>, <year>2024</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIFS.2023.3329426">10.1109/TIFS.2023.3329426</ext-link>.</mixed-citation></ref>
<ref id="ref-5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names> <surname>Ladbury</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>Utilization of modelagnostic explainable artificial intelligence frameworks in oncology: A narrative review</article-title>,&#x201D; <source>Transl. Cancer Res.</source>, vol. <volume>11</volume>, no. <issue>10</issue>, pp. <fpage>3853</fpage>, <year>2022</year>.</mixed-citation></ref>
<ref id="ref-6"><label>6</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Farhadloo</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>Samcnet: Towards a spatially explainable AI approach for classifying MXIF oncology data</article-title>,&#x201D; in <conf-name>Proc. 28th ACM SIGKDD Conf. Knowl. Discov. Data Min.</conf-name>, <publisher-loc>Washington DC, USA</publisher-loc>, <year>Aug. 2022</year>, pp. <fpage>2860</fpage>&#x2013;<lpage>2870</lpage>.</mixed-citation></ref>
<ref id="ref-7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Almutiq</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zhao</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Ren</surname></string-name></person-group>, &#x201C;<article-title>Distributed medical data storage mechanism based on proof of retrievability and vector commitment for metaverse services<!--Q1: Reference &#x2018;7&#x2019; is incomplete. Please provide any of the relevant missing information: volume, page range.--></article-title>,&#x201D; <source>IEEE J. Biomed. Health Inform.</source>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/JBHI.2023.3272021">10.1109/JBHI.2023.3272021</ext-link>.</mixed-citation></ref>
<ref id="ref-8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Ram&#x00ED;rez-Mena</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Andr&#x00E9;s-Le&#x00F3;n</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Alvarez-Cubero</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Anguita-Ruiz</surname></string-name>, <string-name><given-names>L. J.</given-names> <surname>Martinez-Gonzalez</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Alcala-Fdez</surname></string-name></person-group>, &#x201C;<article-title>Explainable artificial intelligence to predict and identify prostate cancer tissue by gene expression</article-title>,&#x201D; <source>Comput. Methods Programs Biomed.</source>, vol. <volume>240</volume>, no. <issue>2</issue>, pp. <fpage>107719</fpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cmpb.2023.107719">10.1016/j.cmpb.2023.107719</ext-link>.</mixed-citation></ref>
<ref id="ref-9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names> <surname>Khened</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kori</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Rajkumar</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Krishnamurthi</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Srinivasan</surname></string-name></person-group>, &#x201C;<article-title>A generalized deep learning framework for whole-slide image segmentation and analysis</article-title>,&#x201D; <source>Sci. Rep.</source>, vol. <volume>11</volume>, no. <issue>1</issue>, pp. <fpage>11579</fpage>, <year>2021</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-021-90444-8">10.1038/s41598-021-90444-8</ext-link>.</mixed-citation></ref>
<ref id="ref-10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Alkhalaf</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>Adaptive aquila optimizer with explainable artificial intelligence-enabled cancer diagnosis on medical imaging</article-title>,&#x201D; <source>Cancers</source>, vol. <volume>15</volume>, no. <issue>5</issue>, pp. <fpage>1492</fpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3390/cancers15051492">10.3390/cancers15051492</ext-link>.</mixed-citation></ref>
<ref id="ref-11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. R.</given-names> <surname>Djavanshir</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Chen</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Yang</surname></string-name></person-group>, &#x201C;<article-title>A review of artificial intelligence&#x2019;s neural networks (deep learning) applications in medical diagnosis and prediction</article-title>,&#x201D; <source>IT Prof.</source>, vol. <volume>23</volume>, no. <issue>3</issue>, pp. <fpage>58</fpage>&#x2013;<lpage>62</lpage>, <year>2021</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/MITP.2021.3073665">10.1109/MITP.2021.3073665</ext-link>.</mixed-citation></ref>
<ref id="ref-12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. L. D.</given-names> <surname>Ara&#x00FA;jo</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>Machine learning concepts applied to oral pathology and oral medicine: A convolutional neural networks&#x2019; approach</article-title>,&#x201D; <source>J. Oral Pathol. Med.</source>, vol. <volume>52</volume>, no. <issue>2</issue>, pp. <fpage>109</fpage>&#x2013;<lpage>118</lpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/jop.13397">10.1111/jop.13397</ext-link>.</mixed-citation></ref>
<ref id="ref-13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names> <surname>Chiwariro</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Julius</surname></string-name></person-group>, &#x201C;<article-title>Comparative analysis of deep learning convolutional neural networks based on transfer learning for pneumonia detection</article-title>,&#x201D; <source>Int. J. Res. Appl. Sci. Eng. Technol.</source>, vol. <volume>11</volume>, no. <issue>1</issue>, pp. <fpage>1161</fpage>&#x2013; <lpage>1170</lpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.22214/ijraset.2023.48685">10.22214/ijraset.2023.48685</ext-link>.</mixed-citation></ref>
<ref id="ref-14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. S.</given-names> <surname>An</surname></string-name>, <string-name><given-names>P. N.</given-names> <surname>Lan</surname></string-name>, <string-name><given-names>D. V.</given-names> <surname>Hang</surname></string-name>, <string-name><given-names>D. V.</given-names> <surname>Long</surname></string-name>, <string-name><given-names>T. Q.</given-names> <surname>Trung</surname></string-name>, <string-name><given-names>N. T.</given-names> <surname>Thuy</surname></string-name>, and <string-name><given-names>D. V.</given-names> <surname>Sang</surname></string-name></person-group>, &#x201C;<article-title>Blazeneo: Blazing fast polyp segmentation and neoplasm detection</article-title>,&#x201D; <source>IEEE Access</source>, vol. <volume>10</volume>, pp. <fpage>43669</fpage>&#x2013;<lpage>43684</lpage>, <year>2022</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ACCESS.2022.3168693">10.1109/ACCESS.2022.3168693</ext-link>.</mixed-citation></ref>
<ref id="ref-15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names> <surname>Roszkowiak</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Korzy&#x0144;ska</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Pijanowska</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Bosch</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lejeune</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>L&#x00F3;pez</surname></string-name></person-group>, &#x201C;<article-title>Clustered nuclei splitting based on recurrent distance transform in digital pathology images</article-title>,&#x201D; <source>EURASIP J. Image Video Proc.</source>, vol. <volume>2020</volume>, no. <issue>1</issue>, pp. <fpage>125</fpage>, <year>2020</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13640-020-00514-6">10.1186/s13640-020-00514-6</ext-link>.</mixed-citation></ref>
<ref id="ref-16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names> <surname>Otsu</surname></string-name></person-group>, &#x201C;<article-title>A threshold selection method from gray-level histograms</article-title>,&#x201D; <source>IEEE Trans. Syst., Man, Cybern.</source>, vol. <volume>9</volume>, no. <issue>1</issue>, pp. <fpage>62</fpage>&#x2013; <lpage>66</lpage>, <year>1979</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TSMC.1979.4310076">10.1109/TSMC.1979.4310076</ext-link>.</mixed-citation></ref>
<ref id="ref-17"><label>17</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>S.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Xia</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Zhang</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Belloulata</surname></string-name></person-group>, &#x201C;<article-title>An image segmentation algorithm in image processing based on threshold segmentation</article-title>,&#x201D; in <conf-name>2007 Third Int. IEEE Conf. Signal-Image Technol. Internet-Based Syst.</conf-name>, <publisher-loc>Shanghai, China</publisher-loc>, <year>Dec. 2007</year>, pp. <fpage>673</fpage>&#x2013;<lpage>678</lpage>.</mixed-citation></ref>
<ref id="ref-18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Zheng</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Qingge</surname></string-name></person-group>, &#x201C;<article-title>An improved otsu threshold segmentation algorithm</article-title>,&#x201D; <source>Int. J. Comput. Sci. Eng.</source>, vol. <volume>22</volume>, no. <issue>1</issue>, pp. <fpage>146</fpage>&#x2013;<lpage>153</lpage>, <year>2020</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1504/IJCSE.2020.107266">10.1504/IJCSE.2020.107266</ext-link>.</mixed-citation></ref>
<ref id="ref-19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names> <surname>Guo</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Tondi</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Barni</surname></string-name></person-group>, &#x201C;<article-title>An overview of backdoor attacks against deep neural networks and possible defences</article-title>,&#x201D; <source>IEEE Open J. Signal Process.</source>, vol. <volume>3</volume>, pp. <fpage>261</fpage>&#x2013;<lpage>287</lpage>, <year>2022</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/OJSP.2022.3190213">10.1109/OJSP.2022.3190213</ext-link>.</mixed-citation></ref>
<ref id="ref-20"><label>20</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>K. G. V.</given-names> <surname>Kiran</surname></string-name> and <string-name><given-names>G. M.</given-names> <surname>Reddy</surname></string-name></person-group>, &#x201C;<article-title>Automatic classification of whole slide pap smear images using cnn with pca based feature interpretation</article-title>,&#x201D; in <conf-name>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops</conf-name>, <publisher-loc>Long Beach, CA, USA</publisher-loc>, <year>Jun. 2019</year>.</mixed-citation></ref>
<ref id="ref-21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Wang</surname></string-name>, and <string-name><given-names>P. -A.</given-names> <surname>Heng</surname></string-name></person-group>, &#x201C;<article-title>Dualpath network with synergistic grouping loss and evidence driven risk stratification for whole slide cervical image analysis</article-title>,&#x201D; <source>Med. Image Anal.</source>, vol. <volume>69</volume>, pp. <fpage>101955</fpage>, <year>2021</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.media.2021.101955">10.1016/j.media.2021.101955</ext-link>.</mixed-citation></ref>
<ref id="ref-22"><label>22</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><given-names>A.</given-names> <surname>Dosovitskiy</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title>,&#x201D; <comment>arXiv preprint arXiv:2010.11929</comment>, <year>2020</year>.</mixed-citation></ref>
<ref id="ref-23"><label>23</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>R. J.</given-names> <surname>Chen</surname></string-name> <etal>et al.</etal></person-group>, &#x201C;<article-title>Multimodal co-attention transformer for survival prediction in gigapixel whole slide images</article-title>,&#x201D; in <conf-name>Proc. IEEE/CVF Int. Conf. Comput. Vis.</conf-name>, <year>2021</year>, pp. <fpage>4015</fpage>&#x2013;<lpage>4025</lpage>.</mixed-citation></ref>
<ref id="ref-24"><label>24</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>P.</given-names> <surname>Yin</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Jiang</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Chen</surname></string-name></person-group>, &#x201C;<article-title>Pyramid tokens-to-token vision transformer for thyroid pathology image classification</article-title>,&#x201D; in <conf-name>2022 Eleventh Int. Conf. Image Process. Theory, Tools Appl. (IPTA)</conf-name>, <publisher-name>IEEE</publisher-name>, <year>2022</year>, pp. <fpage>1</fpage>&#x2013;<lpage>6</lpage>.</mixed-citation></ref>
<ref id="ref-25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>V.</given-names> <surname>Badrinarayanan</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Kendall</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Cipolla</surname></string-name></person-group>, &#x201C;<article-title>Segnet: A deep convolutional encoder-decoder architecture for image segmentation</article-title>,&#x201D; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>, vol. <volume>39</volume>, no. <issue>12</issue>, pp. <fpage>2481</fpage>&#x2013;<lpage>2495</lpage>, <year>2017</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2016.2644615">10.1109/TPAMI.2016.2644615</ext-link>.</mixed-citation></ref>
<ref id="ref-26"><label>26</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>Z.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Rahman Siddiquee</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Tajbakhsh</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Liang</surname></string-name></person-group>, &#x201C;<article-title>Unet&#x002B;&#x002B;: A nested u-net architecture for medical image segmentation</article-title>,&#x201D; in <conf-name>Deep Learn. Med. Image Anal. Multimodal Learn. Clinical Decis. Support: 4th Int. Workshop, DLMIA 2018, and 8th Int. Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018 Granada, Spain, September 20, 2018, Proc. 4</conf-name>, <year>2018</year>, pp. <fpage>3</fpage>&#x2013;<lpage>11</lpage>.</mixed-citation></ref>
<ref id="ref-27"><label>27</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>L. -C.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Papandreou</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Schroff</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Adam</surname></string-name></person-group>, &#x201C;<article-title>Encoder decoder with atrous separable convolution for semantic image segmentation</article-title>,&#x201D; in <conf-name>Proc. Eur. Conf. Comput. Vis. (ECCV)</conf-name>, <publisher-loc>Munich, Germany</publisher-loc>, <year>2018</year>, pp. <fpage>801</fpage>&#x2013;<lpage>818</lpage>.</mixed-citation></ref>
<ref id="ref-28"><label>28</label><mixed-citation publication-type="conf-proc"><person-group person-group-type="author"><string-name><given-names>J.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Xiong</surname></string-name>, and <string-name><given-names>S. P.</given-names> <surname>Bhattacharyya</surname></string-name></person-group>, &#x201C;<article-title>Pidnet: A real-time semantic segmentation network inspired by pid controllers</article-title>,&#x201D; in <conf-name>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.</conf-name>, <publisher-loc>Vancouver, Canada</publisher-loc>, <year>Jun. 2023</year>, pp. <fpage>19529</fpage>&#x2013;<lpage>19539</lpage>.</mixed-citation></ref>
<ref id="ref-29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. R.</given-names> <surname>Prusty</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Dinesh</surname></string-name>, <string-name><given-names>H. S. K.</given-names> <surname>Sheth</surname></string-name>, <string-name><given-names>A. L.</given-names> <surname>Viswanath</surname></string-name>, and <string-name><given-names>S. K.</given-names> <surname>Satapathy</surname></string-name></person-group>, &#x201C;<article-title>Nuclei segmentation in histopathology images using structure-preserving color normalization based ensemble deep learning frameworks.</article-title>,&#x201D; <source>Comput. Mater. Contin.</source>, vol. <volume>77</volume>, no. <issue>3</issue>, pp. <fpage>3077</fpage>&#x2013;<lpage>3094</lpage>, <year>2023</year>. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.32604/cmc.2023.042718">10.32604/cmc.2023.042718</ext-link>.</mixed-citation></ref>
</ref-list>
</back>
</article>
